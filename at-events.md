---

copyright:
  years: 2016, 2020
lastupdated: "2020-11-2"

keywords: IBM Event Streams, Kafka as a service, managed Apache Kafka, activity

subcollection: EventStreams

---

{:shortdesc: .shortdesc}
{:new_window: target="_blank"}
{:codeblock: .codeblock}
{:pre: .pre}
{:screen: .screen}
{:tip: .tip}
{:download: .download}
{:table: .aria-labeledby="caption"}

<!-- Name your file `at-events.md` and include it in the Reference nav group in your toc file. -->

# {{site.data.keyword.cloudaccesstrailshort}} events 
{: #at_events}

Use the {{site.data.keyword.cloudaccesstrailfull}} service to track how users and applications interact with the {{site.data.keyword.messagehub}} service on the Standard and Enterprise plans in {{site.data.keyword.Bluemix}}. 
{: shortdesc}

The {{site.data.keyword.cloudaccesstrailfull_notm}} service records user-initiated activities that change the state of a service in {{site.data.keyword.Bluemix_notm}}. For more information, see the [{{site.data.keyword.cloudaccesstrailshort}} ![External link icon](../../icons/launch-glyph.svg "External link icon")](/docs/Activity-Tracker-with-LogDNA?topic=logdnaat-getting-started#getting-started){:new_window}.

<!-- You can create different sections to group events by area. -->

## List of events
{: #events}

<!-- Make sure you introduce the table with a detailed description that immediately precedes it. For example, see https://cloud.ibm.com/docs/cloud-activity-tracker?topic=cloud-activity-tracker-cf#catalog. -->

{{site.data.keyword.messagehub}} on the Standard and Enterprise plans automatically generate events so that you can track activity on your service. Enterprise plan supports all events in the list, Standard plan only supports topic events.

| Action | Description |
|:-------|:------------|
| event-streams.topic.create | An event is created when you create a topic|
| event-streams.topic.delete | An event is created when you delete a topic|
| event-streams.topic.update | An event is created when you update a topic's configuration or increase partitions|
| event-streams.message.read | An event is created when message audit is enabled on a topic and a consumer is reading data from this topic|
| event-streams.message.write | An event is created when message audit is enabled on a topic and a producer is writing data to this topic|
| event-streams.message.delete | An event is created when message audit is enabled and on a topic and records are deleted from this topic's partitions|
| event-streams.storage-key.read | An event is created when access to the disk encryption key in {{site.data.keyword.keymanagementserviceshort}} has changed.</br> If the outcome of this event is <code>success</code>, access to the disk encryption key has been restored and the {{site.data.keyword.messagehub}} instance is available for use.</br> If the outcome is <code>failure</code>, access to the disk encryption key has been withdrawn and the {{site.data.keyword.messagehub}} instance is not available for use. |
| event-streams.storage-key.update | The disk encryption key in {{site.data.keyword.keymanagementserviceshort}} has been rotated and the {{site.data.keyword.messagehub}} instance has been updated to use the new key. |
| event-streams.schema.create | A schema  or schema version has been created or updated in the {{site.data.keyword.messagehub}} schema registry for the enterprise instance either through the administration API or through the Confluent Serdes.
| event-streams.schema.delete | A schema or schema version has been deleted from the {{site.data.keyword.messagehub}} schema registry for the enterprise instance|
| event-streams.schema-rule.create | A new rule or global rule has been created in the {{site.data.keyword.messagehub}} schema registry for the enterprise instance|
| event-streams.schema-rule.update | An existing rule or global rule has been updated in the {{site.data.keyword.messagehub}} schema registry for the enterprise instance|
| event-streams.schema-rule.delete | A rule has been deleted in the {{site.data.keyword.messagehub}} schema registry for the enterprise instance|
|
{: caption="Table 1. {{site.data.keyword.messagehub}} events" caption-side="top"}
<!-- 03/09/19 Karen: kafka.scale.down and kafka.scale.up are both related to BYOK. -->

## Where to view the events
{: #ui}

<!-- For example, choose one of the following two options. -->

<!-- Option 2: Add the following sentence if your service sends events to the account domain. -->

{{site.data.keyword.cloudaccesstrailshort}} events are available in the {{site.data.keyword.cloudaccesstrailshort}} **account domain** that is available in the {{site.data.keyword.Bluemix_notm}} location (region) where the events are generated.

Events that are generated by an instance of the {{site.data.keyword.messagehub}} service are automatically forwarded to the {{site.data.keyword.at_full_notm}} service instance that is available in the same location.

{{site.data.keyword.at_full_notm}} can have only one instance per location. To view events, you must access the web UI of the {{site.data.keyword.at_full_notm}} service in the same location where your service instance is available. For more information, see [Launching the web UI through the {{site.data.keyword.Bluemix_notm}} UI ![External link icon](../../icons/launch-glyph.svg "External link icon")](/docs/Activity-Tracker-with-LogDNA?topic=logdnaat-launch#launch_step2){:new_window}.


## How to enable message audit events
{: #ui}
Message audit events are configured per topic basis, follow below steps to enable it.

1. Install {{site.data.keyword.messagehub}} CLI plugin v2.3 or above: 
   ```
   ibmcloud plugin install event-streams -v 2.3
   ```
2. Enable message audit config on a topic:
   ```
   ibmcloud es topic-update <topic-name> --config message.audit.enable=true
   ```
Note: after topic's message audit config is updated, it takes 5 minutes for message audit events showing up in {{site.data.keyword.cloudaccesstrailshort}}.

Once a topic is configured to enable message audit events, 3 additional types of events are emitted besides the topic events.

`event-streams.message.read`: when a consumer is reading data from a topic

`event-streams.message.write`: when a producer is writing data to a topic

`event-streams.message.delete`: when records are deleted from a topic's partition

Since kafka data is read and write at a very high rate, not every single request will result an event. Instead we aggregate events in such a way: requests from the same location(eg. IP address), the same operation(eg. read, write, delete), against the same topic within 1 hour will result 1 message event.

Additionally, be aware of the implications of enabling message audit events:

1. An internal Kafka topic is used for streaming the events from cluster to {{site.data.keyword.cloudaccesstrailshort}}, thus it takes a small amount of network bandwidth in the kafka cluster. Depending on the cluster's workload, usually the throughput of this topic will not exceed 1KB/s.
   
2. Since more events are sent to {{site.data.keyword.cloudaccesstrailshort}}, extra storage cost incurs for {{site.data.keyword.cloudaccesstrailshort}}. Each event's size is about 1KB, a rough estimation of how much storage it takes: assuming cluster has 100 topics, each topic has 10 clients actively producing and consuming, each client runs on 3 different locations, then it generates 100x10x3=3000 events per hour, that is 2GB per month.